{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron POI Classifier Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing and checking versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\damia\\OneDrive\\Dokumente\\Udacity\\DataAnalyst\\EnronFinal\\final_project\n",
      "sklearn 0.17.1\n",
      "python 2.7.12 |Anaconda 4.1.1 (64-bit)| (default, Jun 29 2016, 11:07:13) [MSC v.1500 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\damia\\OneDrive\\Dokumente\\Udacity\\DataAnalyst\\EnronFinal\\final_project\n",
    "import sys\n",
    "import sklearn\n",
    "print \"sklearn\", sklearn.__version__\n",
    "print \"python\", sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the cleaned data (see EDA document for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "with open(\"clean_data.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect labels and features in appropriate arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_feature_names():\n",
    "    global feature_names\n",
    "    feature_names = sorted(data_dict['ALLEN PHILLIP K'].keys())\n",
    "    feature_names.remove('poi') # the label\n",
    "    feature_names.remove('email_address') # unique for each data point, thus exclude\n",
    "    feature_names.remove('total_payments') # linear combination of other features\n",
    "    feature_names.remove('total_stock_value') # linear combination of other features\n",
    "def update_data():\n",
    "    global labels, features\n",
    "    labels = [1.0 if d['poi'] else 0.0 for d in data_dict.values()]\n",
    "    features = [[float(d[f]) if f in d and d[f] != 'NaN' else 0.0 for f in feature_names] for d in data_dict.values()]\n",
    "init_feature_names()\n",
    "update_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize features for use in models that are sensitive to magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_feature_values(fidx):\n",
    "    return [features[i][fidx] for i in range(len(features))]\n",
    "mu_f = [np.mean(get_feature_values(fidx)) for fidx in range(len(feature_names))]\n",
    "sd_f = [np.std(get_feature_values(fidx)) for fidx in range(len(feature_names))]\n",
    "norm_features = []\n",
    "for idx in range(len(features)):\n",
    "    data = []\n",
    "    for fidx in range(len(feature_names)):\n",
    "        data.append((features[idx][fidx] - mu_f[fidx]) / sd_f[fidx] if sd_f[fidx] != 0 else features[idx][fidx])\n",
    "    norm_features.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import various stuff needed for model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exercised_stock_options  : 0.198 0.198\n",
      "fraction_to_poi          : 0.149 0.348\n",
      "other                    : 0.124 0.471\n",
      "shared_receipt_with_poi  : 0.111 0.583\n",
      "expenses                 : 0.107 0.690\n",
      "bonus                    : 0.096 0.786\n",
      "restricted_stock         : 0.041 0.827\n",
      "long_term_incentive      : 0.036 0.863\n",
      "deferred_income          : 0.036 0.899\n",
      "salary                   : 0.021 0.920\n",
      "from_messages            : 0.019 0.940\n",
      "from_this_person_to_poi  : 0.019 0.958\n",
      "deferral_payments        : 0.011 0.969\n",
      "fraction_from_poi        : 0.011 0.980\n",
      "from_poi_to_this_person  : 0.010 0.989\n",
      "to_messages              : 0.009 0.998\n",
      "loan_advances            : 0.001 0.999\n",
      "restricted_stock_deferred: 0.001 1.000\n",
      "director_fees            : 0.000 1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "splitter = StratifiedShuffleSplit(labels, n_iter=1000, random_state=42)\n",
    "scores = {}\n",
    "for name in feature_names:\n",
    "    scores[name] = []\n",
    "for idx_train, idx_test in splitter:\n",
    "    features_train = [features[i] for i in idx_train]\n",
    "    features_test = [features[i] for i in idx_test]\n",
    "    labels_train = [labels[i] for i in idx_train]\n",
    "    labels_test = [labels[i] for i in idx_test]\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(features_train, labels_train)\n",
    "    for i in range(len(feature_names)):\n",
    "        scores[feature_names[i]].append(clf.feature_importances_[i])\n",
    "for name, values in scores.items():\n",
    "    scores[name] = np.mean(values)\n",
    "cumulative_score = 0\n",
    "for name, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    cumulative_score += score\n",
    "    print \"%-25s: %.3f %.3f\" % (name, score, cumulative_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I select the top 5 features for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = ['exercised_stock_options', 'fraction_to_poi', 'other', 'shared_receipt_with_poi', 'expenses']\n",
    "update_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.513\n",
      "Parameters:\n",
      "class_weight: None\n",
      "criterion: 'entropy'\n",
      "max_depth: 4\n",
      "max_features: None\n",
      "max_leaf_nodes: None\n",
      "min_samples_leaf: 4\n",
      "min_samples_split: 3\n",
      "min_weight_fraction_leaf: 0.0\n",
      "presort: False\n",
      "random_state: None\n",
      "splitter: 'best'\n"
     ]
    }
   ],
   "source": [
    "parameters = {'max_depth': (1, 2, 3, 4, 5, None),\n",
    "              'min_samples_split': (2, 3, 4),\n",
    "              'min_samples_leaf': (1, 2, 3, 4),\n",
    "              'max_leaf_nodes': (2, 3, 4, 5, 10, None),\n",
    "              'criterion': ('gini', 'entropy')\n",
    "             }\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), \n",
    "                   parameters, \n",
    "                   scoring='f1', \n",
    "                   cv=StratifiedShuffleSplit(labels, n_iter=100, random_state=42))\n",
    "clf.fit(features, labels)\n",
    "print 'Score: %0.3f' % clf.best_score_\n",
    "print 'Parameters:'\n",
    "for name, value in sorted(clf.best_estimator_.get_params().items()):\n",
    "    print '%s: %r' % (name, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from_messages              : -31.1839\n",
      "restricted_stock_deferred  :  11.7534\n",
      "from_this_person_to_poi    :   9.6442\n",
      "director_fees              :  -4.8050\n",
      "deferral_payments          :  -3.2756\n",
      "deferred_income            :  -2.5946\n",
      "from_poi_to_this_person    :   1.5348\n",
      "salary                     :   1.1790\n",
      "bonus                      :  -1.1330\n",
      "exercised_stock_options    :   0.8050\n",
      "fraction_from_poi          :  -0.8016\n",
      "expenses                   :   0.7647\n",
      "other                      :   0.7273\n",
      "to_messages                :   0.4015\n",
      "loan_advances              :  -0.3558\n",
      "long_term_incentive        :  -0.3376\n",
      "restricted_stock           :  -0.1765\n",
      "fraction_to_poi            :   0.0921\n",
      "shared_receipt_with_poi    :   0.0394\n"
     ]
    }
   ],
   "source": [
    "init_feature_names()\n",
    "update_data()\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "clf = LogisticRegressionCV(cv=StratifiedShuffleSplit(labels, n_iter=1000, random_state=42),\n",
    "                           scoring='f1',\n",
    "                           solver='liblinear',\n",
    "                           penalty='l1')\n",
    "clf.fit(norm_features, labels)\n",
    "coefs = [(feature_names[idx], clf.coef_[0][idx]) for idx in range(len(feature_names))]\n",
    "for name, value in sorted(coefs, key=lambda x: abs(x[1]), reverse=True):\n",
    "    print '%-27s: %8.4f' % (name, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the 9 top features as they have the largest weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = ['from_messages', 'restricted_stock_deferred', 'from_this_person_to_poi', 'director_fees', \n",
    "                 'deferral_payments', 'deferred_income', 'from_poi_to_this_person', 'salary', 'bonus']\n",
    "update_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.441\n",
      "Parameters:\n",
      "C: 0.25\n",
      "class_weight: None\n",
      "dual: False\n",
      "fit_intercept: False\n",
      "intercept_scaling: 1\n",
      "max_iter: 100\n",
      "multi_class: 'ovr'\n",
      "n_jobs: 1\n",
      "penalty: 'l1'\n",
      "random_state: None\n",
      "solver: 'liblinear'\n",
      "tol: 1e-05\n",
      "verbose: 0\n",
      "warm_start: False\n"
     ]
    }
   ],
   "source": [
    "parameters = {'solver': ('liblinear',),\n",
    "              'penalty': ('l1',),\n",
    "              'fit_intercept': (True, False),\n",
    "              'C': (.25, .5, .9, 1., 2.),\n",
    "              'tol': (1e-5, 1e-4, 1e-3)\n",
    "             }\n",
    "clf = GridSearchCV(LogisticRegression(), \n",
    "                   parameters, \n",
    "                   scoring='f1', \n",
    "                   cv=StratifiedShuffleSplit(labels, n_iter=100, random_state=42))\n",
    "clf.fit(norm_features, labels)\n",
    "print 'Score: %0.3f' % clf.best_score_\n",
    "print 'Parameters:'\n",
    "for name, value in sorted(clf.best_estimator_.get_params().items()):\n",
    "    print '%s: %r' % (name, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model selection\n",
    "\n",
    "As the decision tree has a higher F1 score (0.513) than the logistic regression model (0.441), I choose the decision tree model as the final model. In addition to this, it uses less features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
